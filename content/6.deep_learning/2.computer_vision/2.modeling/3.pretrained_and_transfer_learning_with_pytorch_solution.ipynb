{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained and transfer learning with PyTorch\n",
    "\n",
    "You've seen how to implement pre-trained networks and use transfer learning with Keras in [the previous notebook](./2.pretrained_and_transfer_learning_with_keras.ipynb), now it's time to do the same using `pytorch`. The idea and theory is still the same, only the implementation differs.\n",
    "\n",
    "## 1. Pre-trained model usage\n",
    "\n",
    "In this chapter, we will learn how to import a **PyTorch** model that has been pre-trained to recognize various objects in full-color images.\n",
    "\n",
    "### 1.1 Importing an existing model\n",
    "\n",
    "Head on over to the official [torchvision webpage](https://pytorch.org/vision/stable/models.html) to check out what models are there to pick and choose from for our object recognition task.\n",
    "\n",
    "For this exercise, **you** can choose whatever network you want. Look online what models are often used, or look at the evaluation metrics on the given webpage to determine which one is fit for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 1280, 7, 7]          --\n",
      "|    └─ConvBNReLU: 2-1                   [-1, 32, 112, 112]        --\n",
      "|    |    └─Conv2d: 3-1                  [-1, 32, 112, 112]        864\n",
      "|    |    └─BatchNorm2d: 3-2             [-1, 32, 112, 112]        64\n",
      "|    |    └─ReLU6: 3-3                   [-1, 32, 112, 112]        --\n",
      "|    └─InvertedResidual: 2-2             [-1, 16, 112, 112]        --\n",
      "|    |    └─Sequential: 3-4              [-1, 16, 112, 112]        896\n",
      "|    └─InvertedResidual: 2-3             [-1, 24, 56, 56]          --\n",
      "|    |    └─Sequential: 3-5              [-1, 24, 56, 56]          5,136\n",
      "|    └─InvertedResidual: 2-4             [-1, 24, 56, 56]          --\n",
      "|    |    └─Sequential: 3-6              [-1, 24, 56, 56]          8,832\n",
      "|    └─InvertedResidual: 2-5             [-1, 32, 28, 28]          --\n",
      "|    |    └─Sequential: 3-7              [-1, 32, 28, 28]          10,000\n",
      "|    └─InvertedResidual: 2-6             [-1, 32, 28, 28]          --\n",
      "|    |    └─Sequential: 3-8              [-1, 32, 28, 28]          14,848\n",
      "|    └─InvertedResidual: 2-7             [-1, 32, 28, 28]          --\n",
      "|    |    └─Sequential: 3-9              [-1, 32, 28, 28]          14,848\n",
      "|    └─InvertedResidual: 2-8             [-1, 64, 14, 14]          --\n",
      "|    |    └─Sequential: 3-10             [-1, 64, 14, 14]          21,056\n",
      "|    └─InvertedResidual: 2-9             [-1, 64, 14, 14]          --\n",
      "|    |    └─Sequential: 3-11             [-1, 64, 14, 14]          54,272\n",
      "|    └─InvertedResidual: 2-10            [-1, 64, 14, 14]          --\n",
      "|    |    └─Sequential: 3-12             [-1, 64, 14, 14]          54,272\n",
      "|    └─InvertedResidual: 2-11            [-1, 64, 14, 14]          --\n",
      "|    |    └─Sequential: 3-13             [-1, 64, 14, 14]          54,272\n",
      "|    └─InvertedResidual: 2-12            [-1, 96, 14, 14]          --\n",
      "|    |    └─Sequential: 3-14             [-1, 96, 14, 14]          66,624\n",
      "|    └─InvertedResidual: 2-13            [-1, 96, 14, 14]          --\n",
      "|    |    └─Sequential: 3-15             [-1, 96, 14, 14]          118,272\n",
      "|    └─InvertedResidual: 2-14            [-1, 96, 14, 14]          --\n",
      "|    |    └─Sequential: 3-16             [-1, 96, 14, 14]          118,272\n",
      "|    └─InvertedResidual: 2-15            [-1, 160, 7, 7]           --\n",
      "|    |    └─Sequential: 3-17             [-1, 160, 7, 7]           155,264\n",
      "|    └─InvertedResidual: 2-16            [-1, 160, 7, 7]           --\n",
      "|    |    └─Sequential: 3-18             [-1, 160, 7, 7]           320,000\n",
      "|    └─InvertedResidual: 2-17            [-1, 160, 7, 7]           --\n",
      "|    |    └─Sequential: 3-19             [-1, 160, 7, 7]           320,000\n",
      "|    └─InvertedResidual: 2-18            [-1, 320, 7, 7]           --\n",
      "|    |    └─Sequential: 3-20             [-1, 320, 7, 7]           473,920\n",
      "|    └─ConvBNReLU: 2-19                  [-1, 1280, 7, 7]          --\n",
      "|    |    └─Conv2d: 3-21                 [-1, 1280, 7, 7]          409,600\n",
      "|    |    └─BatchNorm2d: 3-22            [-1, 1280, 7, 7]          2,560\n",
      "|    |    └─ReLU6: 3-23                  [-1, 1280, 7, 7]          --\n",
      "├─Sequential: 1-2                        [-1, 1000]                --\n",
      "|    └─Dropout: 2-20                     [-1, 1280]                --\n",
      "|    └─Linear: 2-21                      [-1, 1000]                1,281,000\n",
      "==========================================================================================\n",
      "Total params: 3,504,872\n",
      "Trainable params: 3,504,872\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 158.62\n",
      "==========================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 15.82\n",
      "Params size (MB): 13.37\n",
      "Estimated Total Size (MB): 29.77\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [-1, 1280, 7, 7]          --\n",
       "|    └─ConvBNReLU: 2-1                   [-1, 32, 112, 112]        --\n",
       "|    |    └─Conv2d: 3-1                  [-1, 32, 112, 112]        864\n",
       "|    |    └─BatchNorm2d: 3-2             [-1, 32, 112, 112]        64\n",
       "|    |    └─ReLU6: 3-3                   [-1, 32, 112, 112]        --\n",
       "|    └─InvertedResidual: 2-2             [-1, 16, 112, 112]        --\n",
       "|    |    └─Sequential: 3-4              [-1, 16, 112, 112]        896\n",
       "|    └─InvertedResidual: 2-3             [-1, 24, 56, 56]          --\n",
       "|    |    └─Sequential: 3-5              [-1, 24, 56, 56]          5,136\n",
       "|    └─InvertedResidual: 2-4             [-1, 24, 56, 56]          --\n",
       "|    |    └─Sequential: 3-6              [-1, 24, 56, 56]          8,832\n",
       "|    └─InvertedResidual: 2-5             [-1, 32, 28, 28]          --\n",
       "|    |    └─Sequential: 3-7              [-1, 32, 28, 28]          10,000\n",
       "|    └─InvertedResidual: 2-6             [-1, 32, 28, 28]          --\n",
       "|    |    └─Sequential: 3-8              [-1, 32, 28, 28]          14,848\n",
       "|    └─InvertedResidual: 2-7             [-1, 32, 28, 28]          --\n",
       "|    |    └─Sequential: 3-9              [-1, 32, 28, 28]          14,848\n",
       "|    └─InvertedResidual: 2-8             [-1, 64, 14, 14]          --\n",
       "|    |    └─Sequential: 3-10             [-1, 64, 14, 14]          21,056\n",
       "|    └─InvertedResidual: 2-9             [-1, 64, 14, 14]          --\n",
       "|    |    └─Sequential: 3-11             [-1, 64, 14, 14]          54,272\n",
       "|    └─InvertedResidual: 2-10            [-1, 64, 14, 14]          --\n",
       "|    |    └─Sequential: 3-12             [-1, 64, 14, 14]          54,272\n",
       "|    └─InvertedResidual: 2-11            [-1, 64, 14, 14]          --\n",
       "|    |    └─Sequential: 3-13             [-1, 64, 14, 14]          54,272\n",
       "|    └─InvertedResidual: 2-12            [-1, 96, 14, 14]          --\n",
       "|    |    └─Sequential: 3-14             [-1, 96, 14, 14]          66,624\n",
       "|    └─InvertedResidual: 2-13            [-1, 96, 14, 14]          --\n",
       "|    |    └─Sequential: 3-15             [-1, 96, 14, 14]          118,272\n",
       "|    └─InvertedResidual: 2-14            [-1, 96, 14, 14]          --\n",
       "|    |    └─Sequential: 3-16             [-1, 96, 14, 14]          118,272\n",
       "|    └─InvertedResidual: 2-15            [-1, 160, 7, 7]           --\n",
       "|    |    └─Sequential: 3-17             [-1, 160, 7, 7]           155,264\n",
       "|    └─InvertedResidual: 2-16            [-1, 160, 7, 7]           --\n",
       "|    |    └─Sequential: 3-18             [-1, 160, 7, 7]           320,000\n",
       "|    └─InvertedResidual: 2-17            [-1, 160, 7, 7]           --\n",
       "|    |    └─Sequential: 3-19             [-1, 160, 7, 7]           320,000\n",
       "|    └─InvertedResidual: 2-18            [-1, 320, 7, 7]           --\n",
       "|    |    └─Sequential: 3-20             [-1, 320, 7, 7]           473,920\n",
       "|    └─ConvBNReLU: 2-19                  [-1, 1280, 7, 7]          --\n",
       "|    |    └─Conv2d: 3-21                 [-1, 1280, 7, 7]          409,600\n",
       "|    |    └─BatchNorm2d: 3-22            [-1, 1280, 7, 7]          2,560\n",
       "|    |    └─ReLU6: 3-23                  [-1, 1280, 7, 7]          --\n",
       "├─Sequential: 1-2                        [-1, 1000]                --\n",
       "|    └─Dropout: 2-20                     [-1, 1280]                --\n",
       "|    └─Linear: 2-21                      [-1, 1000]                1,281,000\n",
       "==========================================================================================\n",
       "Total params: 3,504,872\n",
       "Trainable params: 3,504,872\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 158.62\n",
       "==========================================================================================\n",
       "Input size (MB): 0.57\n",
       "Forward/backward pass size (MB): 15.82\n",
       "Params size (MB): 13.37\n",
       "Estimated Total Size (MB): 29.77\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Make a model of your choice. Make sure you import the pre-trained model, and not just the architecture.\n",
    "model = models.mobilenet_v2()\n",
    "model = model.to(device)\n",
    "# Take a look at the model.\n",
    "input_dim = (3,224, 224)\n",
    "summary(model, input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preparing your images\n",
    "\n",
    "As with Keras, pre-trained models are trained on images that went through a specific preprocessing step. To effectively use the model, we need to go through the right preprocessing steps with our images as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load your image.\n",
    "img = Image.open(\"./../assets/rick.jpg\")\n",
    "\n",
    "# Make a transform object based on your specific model preprocessing transformations.\n",
    "transform = transforms.Compose([           \n",
    "  transforms.Resize(256),                   \n",
    "  transforms.CenterCrop(224),                \n",
    "  transforms.ToTensor(),                     \n",
    "  transforms.Normalize(                      \n",
    "  mean=[0.485, 0.456, 0.406],                \n",
    "  std=[0.229, 0.224, 0.225]                  \n",
    "  )])\n",
    "\n",
    "# Transform your image.\n",
    "img = transform(img)\n",
    "\n",
    "# Prepare a batch for the model to accept.\n",
    "batch = torch.unsqueeze(img, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Predicting the class of your image\n",
    "\n",
    "Getting a torchvision model to work is as easy as pie, but interpreting the results is a bit more involved..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30096\\1464197622.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Predict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Print out the prediction index with the highest likelihood.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yurit\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yurit\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\mobilenet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yurit\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\mobilenet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;31m# This exists since TorchScript doesn't support inheritance, so the superclass method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# (this one) needs to have a name other than `forward` that can be accessed in a subclass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m         \u001b[1;31m# Cannot use \"squeeze\" as batch-size can be 1 => must use reshape with x.shape[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madaptive_avg_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yurit\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yurit\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yurit\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yurit\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yurit\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yurit\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yurit\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 420\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "# Put the model into evaluation mode.\n",
    "model.eval()\n",
    "\n",
    "# Predict.\n",
    "out = model(batch)\n",
    "\n",
    "# Print out the prediction index with the highest likelihood.\n",
    "print(out.max(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly interpret this probability index, you need to look up how your pre-trained model final layer is structured. There are some text files floating out there for most popular pre-trained networks that map the indices of the output nodes to actual labels, but the torchvision models somehow do not come with these equipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transfer learning\n",
    "\n",
    "### 2.1 Importing, preprocessing and augmenting the data\n",
    "\n",
    "As with the Keras exercise, we want to retrain part of the CNN on **hot dogs** and **non hot dogs**. This will effectively re-purpose the feature extraction capabilities of the general models to accurately identify hot dogs.\n",
    "\n",
    "Pytorch's way of augmenting data is also part of the `transforms` sub-library. Augment your data as part of your transform composition to preprocess and augment in one go!\n",
    "\n",
    "If you have not already done so, get your hot dog dataset [here](https://www.kaggle.com/dansbecker/hot-dog-not-hot-dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30096\\2038407912.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# Load data from folders.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m data = {\n\u001b[1;32m---> 41\u001b[1;33m      \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimage_transforms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m      \u001b[1;34m'valid'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimage_transforms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'valid'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m      \u001b[1;34m'test'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimage_transforms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# Make a transform object based on your specific model preprocessing transformations.\n",
    "image_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Set train and valid directory paths.\n",
    "train_directory = \"./../assets/train\"\n",
    "valid_directory = \"./../assets/valid\"\n",
    "test_directory = \"./../assets/test\"\n",
    "\n",
    "# Batch size.\n",
    "bs = 8\n",
    "\n",
    "# Number of classes.\n",
    "num_classes = 2\n",
    "\n",
    "# Load data from folders.\n",
    "data = {\n",
    "     'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
    "     'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid']),\n",
    "     'test': datasets.ImageFolder(root=test_directory, transform=image_transforms['test'])\n",
    " }\n",
    "\n",
    "# Size of data, to be used for calculating average loss and accuracy.\n",
    "train_data_size = len(data['train'])\n",
    "valid_data_size = len(data['valid'])\n",
    "test_data_size = len(data['test'])\n",
    "\n",
    "# Create iterators for the Data loaded using DataLoader module.\n",
    "train_data = DataLoader(data['train'], batch_size=bs, shuffle=True)\n",
    "valid_data = DataLoader(data['valid'], batch_size=bs, shuffle=True)\n",
    "test_data = DataLoader(data['test'], batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Importing part of an existing model\n",
    "\n",
    "As with the Keras notebook, we load in a pre-trained model, and freeze the feature extraction layers so they cannot be retrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Make a model of your choice. Make sure you import the pre-trained model, and not just the architecture.\n",
    "model = models.mobilenet_v2()\n",
    "model = model.to(device)\n",
    "\n",
    "# Freeze model parameters.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Adding flattening and dense layers\n",
    "\n",
    "Now, we'll replace the the final layer with a concoction of our own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the final layer of model for Transfer Learning. Make sure you have only two output classes.\n",
    "model_inputs = model.classifier[1].in_features\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(model_inputs, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, 2),\n",
    "    nn.Softmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "# Create appropriate loss function and optimizer.\n",
    "loss_func = nn.CrossEntropyLoss\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As PyTorch is sometimes a bit involved, the training and validation loop is given to you here for free:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "epochs = 6\n",
    "history = []\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "        # Set to training mode.\n",
    "        model.train()\n",
    "        \n",
    "        # Loss and accuracy within the epoch.\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Clean existing gradients.\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - compute outputs on input data using the model.\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss.\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "\n",
    "            # Backpropagate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute the total loss for the batch and add it to train_loss.\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Compute the accuracy.\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "            \n",
    "            # Convert correct_counts to float and then compute the mean.\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "            # Compute total accuracy in the whole batch and add to train_acc.\n",
    "            train_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "            print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
    "        \n",
    "        # Validation - No gradient tracking needed.\n",
    "        with torch.no_grad():\n",
    "            # Set to evaluation mode.\n",
    "            model.eval()\n",
    "\n",
    "            # Validation loop.\n",
    "            for j, (inputs, labels) in enumerate(valid_data_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass - compute outputs on input data using the model.\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Compute loss.\n",
    "                loss = loss_criterion(outputs, labels)\n",
    "                \n",
    "                # Compute the total loss for the batch and add it to valid_loss.\n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Calculate validation accuracy.\n",
    "                ret, predictions = torch.max(outputs.data, 1)\n",
    "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "                # Convert correct_counts to float and then compute the mean.\n",
    "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "                # Compute total accuracy in the whole batch and add to valid_acc.\n",
    "                valid_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "                print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "\n",
    "        # Find average training loss and training accuracy.\n",
    "        avg_train_loss = train_loss/train_data_size\n",
    "        avg_train_acc = train_acc/float(train_data_size)\n",
    "\n",
    "        # Find average training loss and training accuracy.\n",
    "        avg_valid_loss = valid_loss/valid_data_size\n",
    "        avg_valid_acc = valid_acc/float(valid_data_size)\n",
    "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "        epoch_end = time.time()\n",
    "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, nttValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's test our model one last time using the test set. Here's some helper code to make the testing easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict(model, test_image_name):\n",
    "    transform = image_transforms['test']\n",
    "    test_image = Image.open(test_image_name)\n",
    "    plt.imshow(test_image)\n",
    "    test_image_tensor = transform(test_image)\n",
    "    if torch.cuda.is_available():\n",
    "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224).cuda()\n",
    "    else:\n",
    "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Model outputs log probabilities\n",
    "        out = model(test_image_tensor)\n",
    "        ps = torch.exp(out)\n",
    "        topk, topclass = ps.topk(1, dim=1)\n",
    "        idx_to_class = ['Not a hot dog!', 'Yum, hot dog!']\n",
    "\n",
    "        print(\"Output class :  \", idx_to_class[topclass.cpu().numpy()[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did you do? How many hot dogs did you successfully identify? Aren't you glad you'll never mistake a banana for a hot dog again?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd017e3e2e3c7a6eaf831565dddaa8b1ca329b1d7b31bc84d53ed2d295526c43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
