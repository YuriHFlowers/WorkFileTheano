{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object recognition in pytorch\n",
    "\n",
    "Pytorch is our trusty counterpart to keras/tensorflow, and since both are in use in the world of **deep learning**, we'll let you have a go with pytorch as well!\n",
    "\n",
    "We'll go through the steps pretty quickly, together!\n",
    "\n",
    "## Examining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy -P ../assets/\n",
    "!wget -N https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cat.npy -P ../assets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "dogs = np.load(\"../assets/dog.npy\")\n",
    "cats = np.load(\"../assets/cat.npy\")\n",
    "\n",
    "plt.bar([0,1], [dogs.shape[0], cats.shape[0]])\n",
    "plt.title('dataset sizes')\n",
    "plt.xticks([0,1], ['dogs', 'cats'])\n",
    "plt.ylabel('number of samples');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123202, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raster(image):\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "\n",
    "dog_sample = dogs[0].reshape(28,28)\n",
    "plot_raster(dog_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sample = cats[1].reshape(28,28)\n",
    "plot_raster(cat_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance datasets\n",
    "max_samples = 20000\n",
    "preprocessed_cats = cats[:max_samples].reshape(-1,28,28)\n",
    "preprocessed_dogs = dogs[:max_samples].reshape(-1,28,28)\n",
    "\n",
    "# Normalizing\n",
    "preprocessed_cats = preprocessed_cats/255\n",
    "preprocessed_dogs = preprocessed_dogs/255\n",
    "\n",
    "plt.bar([0,1], [preprocessed_dogs.shape[0], preprocessed_cats.shape[0]])\n",
    "plt.title('dataset sizes')\n",
    "plt.xticks([0,1], ['dogs', 'cats'])\n",
    "plt.ylabel('number of samples');\n",
    "\n",
    "print(preprocessed_dogs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.concatenate((preprocessed_cats, preprocessed_dogs))\n",
    "labels = np.concatenate((np.zeros(len(preprocessed_cats)), np.ones(len(preprocessed_dogs))))\n",
    "\n",
    "# include number of channels for pytorch input (amount, channels, height, width)\n",
    "data = np.expand_dims(data, axis=1)\n",
    "\n",
    "# split data in train/val/test set\n",
    "train_val_data, test_data, train_val_labels, test_labels = train_test_split(\n",
    "    data, \n",
    "    labels,\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    train_val_data, \n",
    "    train_val_labels,\n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# We select the GPU if we have one, else the CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Primary device is {str(device).upper()}\")\n",
    "\n",
    "# pytorch requires data of tensor format, not ndarrays\n",
    "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "val_data = torch.tensor(val_data, dtype=torch.float32)\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing and evaluating the model\n",
    "\n",
    "The final model composed in the [keras object recognition example](./3.object_recognition_with_keras.ipynb) seems to perform pretty well, so let's just remake the structure from there!\n",
    "\n",
    "You'll find the model structure a few cells below. This should give you all the information you need to be able to recreate it. Have a go at it!\n",
    "\n",
    "We'll use a `ReLu()` in the Linear layers too, but we **ARE NOT** using `SoftMax()` in the last layer. We are just leaving it be, without any activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # TO COMPLETE\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TO COMPLETE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architecture should look like this when doing `print(model)`:\n",
    "\n",
    "```\n",
    "ConvNet(\n",
    "  (act): ReLU()\n",
    "  (convlayer1): Sequential(\n",
    "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "    (1): ReLU()\n",
    "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (convlayer2): Sequential(\n",
    "    (0): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "    (1): ReLU()\n",
    "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (denselayer1): Linear(in_features=1568, out_features=8, bias=True)\n",
    "  (denselayer2): Linear(in_features=8, out_features=4, bias=True)\n",
    "  (denselayer3): Linear(in_features=4, out_features=2, bias=True)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like said above, we are **not using a `Softmax()` activation**.\n",
    "Can you find out why ? \n",
    "\n",
    "*Hint: Take a closer look at [`nn.CrossEntropyLoss()`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to run the model for `2` epochs, because it is all that is needed.\n",
    "We are going to `CrossEntropyLoss`, with the Adam optimizer and a `learning_rate` of `0.001` and a `batch_size` of `50`.\n",
    "\n",
    "After initializing the `model`, don't forget to upload it to the device `device` using `model.to(device)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the model, the optimizer and the criterion, let's setup the training and validation loop.\n",
    "We have already defined the general structure of the training loop, you just have to complete the helper methods that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def get_batch(data: torch.tensor, \n",
    "              labels : torch.tensor, \n",
    "              index : int, \n",
    "              batch_size: int, \n",
    "              total_step: int, \n",
    "              device : torch.device)-> Tuple[torch.tensor, torch.tensor]:\n",
    "    \"\"\"\n",
    "    Get a batch of size `batch_size` at the given `index` of the \n",
    "    `data` and `labels`, then upload them to the `device`.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    (torch.tensor, torch.tensor)\n",
    "         - Images fed to the model\n",
    "         - Labels of the images\n",
    "    \"\"\"\n",
    "    \n",
    "    # TO COMPLETE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def forward_pass(model : torch.nn.Module, \n",
    "                 criterion : torch.nn.CrossEntropyLoss, \n",
    "                 images : torch.tensor, \n",
    "                 labels : torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n",
    "    \"\"\"Pass the `images` through the `model` for the forward pass,\n",
    "    and compute the loss between the predicted values and the `labels`\n",
    "    using the defined `criterion`.\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    (torch.tensor, torch.tensor)\n",
    "         - Prediction of the model\n",
    "         - Loss\n",
    "    \"\"\"\n",
    "\n",
    "    # TO COMPLETE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def backward_pass(optimizer : torch.optim.Optimizer, loss : torch.tensor) -> None:\n",
    "    \"\"\"\n",
    "    Propagate the `loss` backward in the model and update the\n",
    "    weights using the `optimizer`.\n",
    "    \"\"\"\n",
    "    # TO COMPLETE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def compute_accuracy(output_data : torch.tensor, labels : torch.tensor) -> float:\n",
    "    \"\"\"\n",
    "    Compute the accuracy between the predicted `output_data`\n",
    "    and the true `labels`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Accuracy of the prediction\n",
    "    \"\"\"\n",
    "    # TO COMPLETE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "total_step = len(train_data)\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ##### TRAINING LOOP #####\n",
    "    for index in range(0, total_step, batch_size):\n",
    "        images, labels = get_batch(train_data, train_labels, index, batch_size, total_step, device)\n",
    "        \n",
    "        # Run the forward pass\n",
    "        output_data, loss = forward_pass(model, criterion, images, labels)\n",
    "        train_loss_list.append(loss.item())\n",
    "\n",
    "        backward_pass(optimizer, loss)\n",
    "\n",
    "        # Track the accuracy\n",
    "        accuracy = compute_accuracy(output_data, labels)\n",
    "        train_acc_list.append(accuracy)\n",
    "        \n",
    "        # Print progress every `nb_batches` batches\n",
    "        nb_batches = 100\n",
    "        if index%(batch_size*nb_batches) == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}],\" \\\n",
    "                    f\"Step [{index + 1}/{total_step}],\" \\\n",
    "                    f\"Loss: {loss.item() :.4f},\" \\\n",
    "                    f\"Accuracy: {accuracy * 100 :.2f}%\")\n",
    "                          \n",
    "\n",
    "    ##### VALIDATION LOOP #####\n",
    "    epoch_val_acc_list = [] # Get the validation accuracy on a per epoch level\n",
    "    epoch_val_loss_list = [] # Get the validation loss on a per epoch level\n",
    "\n",
    "    for index in range(0, len(val_data), batch_size):\n",
    "        images, labels = get_batch(val_data, val_labels, index, batch_size, total_step, device)\n",
    "\n",
    "        output_data, loss = forward_pass(model, criterion, images, labels)\n",
    "        val_loss_list.append(loss.item())\n",
    "        epoch_val_loss_list.append(loss.item())\n",
    "        \n",
    "        # Track the accuracy\n",
    "        accuracy = compute_accuracy(output_data, labels)\n",
    "        val_acc_list.append(accuracy)\n",
    "        epoch_val_acc_list.append(accuracy)\n",
    "        \n",
    "    print(\"VALIDATION SET: \" \\\n",
    "             f\"Loss: {np.mean(epoch_val_loss_list) :.4f},\" \\\n",
    "            f\"Accuracy: {np.mean(epoch_val_acc_list) :.2f}%\" \\\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we defined some helper functions to plot the loss and accuracy, as well as the moving average of it, to reduce the noise in the data. You can adjust the `window` parameter to have the average be more or less smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def moving_average(x, w): \n",
    "    \"\"\"Compute the moving average of `x` in a window of `w`\"\"\"\n",
    "    y_axis = np.convolve(x, np.ones(w), 'valid') / w\n",
    "    \n",
    "    # This is a hacky approach, do not use this in production\n",
    "    x_axis = np.linspace(0, num_epochs, len(y_axis))\n",
    "    return x_axis, y_axis\n",
    "\n",
    "def plot_moving_average(train_list, val_list, epochs, ma_window, title):\n",
    "    plt.figure()\n",
    "    train_x_axis = np.linspace(0, epochs, len(train_list))\n",
    "    val_x_axis = np.linspace(0, epochs, len(val_list))\n",
    "    plt.plot(train_x_axis, train_list, label=\"Training set\")\n",
    "    plt.plot(val_x_axis, val_list, label=\"Validation set\")\n",
    "\n",
    "    avg_train_x, avg_train_y = moving_average(train_list, window)\n",
    "    avg_val_x, avg_val_y = moving_average(val_list, window)\n",
    "\n",
    "    plt.plot(avg_train_x, avg_train_y, label=f\"Moving (train) average (w={window})\")\n",
    "    plt.plot(avg_val_x, avg_val_y,  label=f\"Moving (val) average (w={window})\")\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(title)\n",
    "    plt.legend(loc='best')\n",
    "    lower_limit = 0.7 if title == 'Accuracy' else 0 \n",
    "    plt.ylim(lower_limit, 1)\n",
    "    \n",
    "window = 10 # Change the window to have a smoother curve\n",
    "plot_moving_average(train_acc_list, val_acc_list, num_epochs, window, \"Accuracy\")\n",
    "plot_moving_average(train_loss_list, val_loss_list, num_epochs, window, \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model on the test set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That final test score looks mighty fine, but that training curve looks so jittery! Can you guess why this is? Discuss with your colleagues! (hint: batch sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in a `ckpt` file format\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8d21f3b75a0e2120eb0e66e21d4adea905fc160c040159242535b1a26e9f956d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
