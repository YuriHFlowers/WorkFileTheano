{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained and transfer learning with PyTorch\n",
    "\n",
    "You've seen how to implement pre-trained networks and use transfer learning with Keras in [the previous notebook](./2.pretrained_and_transfer_learning_with_keras.ipynb), now it's time to do the same using `pytorch`. The idea and theory is still the same, only the implementation differs.\n",
    "\n",
    "## 1. Pre-trained model usage\n",
    "\n",
    "In this chapter, we will learn how to import a **PyTorch** model that has been pre-trained to recognize various objects in full-color images.\n",
    "\n",
    "### 1.1 Importing an existing model\n",
    "\n",
    "Head on over to the official [torchvision webpage](https://pytorch.org/vision/stable/models.html) to check out what models are there to pick and choose from for our object recognition task.\n",
    "\n",
    "For this exercise, **you** can choose whatever network you want. Look online what models are often used, or look at the evaluation metrics on the given webpage to determine which one is fit for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "# Make a model of your choice. Make sure you import the pre-trained model, and not just the architecture.\n",
    "# model = ...\n",
    "\n",
    "# Take a look at the model.\n",
    "# input_dim = ...\n",
    "# summary(model, input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preparing your images\n",
    "\n",
    "As with Keras, pre-trained models are trained on images that went through a specific preprocessing step. To effectively use the model, we need to go through the right preprocessing steps with our images as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load your image.\n",
    "# img = Image.open(...\n",
    "\n",
    "# Make a transform object based on your specific model preprocessing transformations.\n",
    "# transform = transforms.Compose([           \n",
    "#  transforms.Resize(...),                   \n",
    "#  transforms.CenterCrop(...),                \n",
    "#  transforms.ToTensor(),                     \n",
    "#  transforms.Normalize(                      \n",
    "#  mean=[...],                \n",
    "#  std=[...]                  \n",
    "#  )])\n",
    "\n",
    "# Transform your image.\n",
    "# img = ...\n",
    "\n",
    "# Prepare a batch for the model to accept.\n",
    "# batch = torch.unsqueeze(img, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Predicting the class of your image\n",
    "\n",
    "Getting a torchvision model to work is as easy as pie, but interpreting the results is a bit more involved..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the model into evaluation mode.\n",
    "model.eval()\n",
    "\n",
    "# Predict.\n",
    "out = model(batch)\n",
    "\n",
    "# Print out the prediction index with the highest likelihood.\n",
    "print(out.max(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly interpret this probability index, you need to look up how your pre-trained model final layer is structured. There are some text files floating out there for most popular pre-trained networks that map the indices of the output nodes to actual labels, but the torchvision models somehow do not come with these equipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transfer learning\n",
    "\n",
    "### 2.1 Importing, preprocessing and augmenting the data\n",
    "\n",
    "As with the Keras exercise, we want to retrain part of the CNN on **hot dogs** and **non hot dogs**. This will effectively re-purpose the feature extraction capabilities of the general models to accurately identify hot dogs.\n",
    "\n",
    "Pytorch's way of augmenting data is also part of the `transforms` sub-library. Augment your data as part of your transform composition to preprocess and augment in one go!\n",
    "\n",
    "If you have not already done so, get your hot dog dataset [here](https://www.kaggle.com/dansbecker/hot-dog-not-hot-dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a transform object based on your specific model preprocessing transformations.\n",
    "#image_transforms = {\n",
    "#    'train': transforms.Compose([\n",
    "#        transforms.RandomResizedCrop(size=..., scale=...,\n",
    "#        transforms.RandomRotation(degrees=...),\n",
    "#        transforms.RandomHorizontalFlip(),\n",
    "#        transforms.Resize(...),\n",
    "#        transforms.CenterCrop(...),\n",
    "#        transforms.ToTensor(),\n",
    "#        transforms.Normalize([...],\n",
    "#                             [...])\n",
    "#    ]),\n",
    "#    'valid': transforms.Compose([\n",
    "#        transforms.Resize(...),\n",
    "#        transforms.CenterCrop(...),\n",
    "#        transforms.ToTensor(),\n",
    "#        transforms.Normalize([...],\n",
    "#                          [...]),\n",
    "#    'test': transforms.Compose([\n",
    "#        transforms.Resize(...),\n",
    "#        transforms.CenterCrop(...),\n",
    "#        transforms.ToTensor(),\n",
    "#        transforms.Normalize([...],\n",
    "#                             [...])\n",
    "#    ])\n",
    "#}\n",
    "\n",
    "# Set train and valid directory paths.\n",
    "# train_directory = ...\n",
    "# valid_directory = ...\n",
    "# test_directory = ...\n",
    "\n",
    "# Batch size\n",
    "bs = 32\n",
    "\n",
    "# Number of classes.\n",
    "# num_classes = ...\n",
    "\n",
    "# Load data from folders.\n",
    " data = {\n",
    "     'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
    "     'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid']),\n",
    "     'test': datasets.ImageFolder(root=test_directory, transform=image_transforms['test'])\n",
    " }\n",
    "\n",
    "# Size of Data, to be used for calculating average loss and accuracy.\n",
    "train_data_size = len(data['train'])\n",
    "valid_data_size = len(data['valid'])\n",
    "test_data_size = len(data['test'])\n",
    "\n",
    "# Create iterators for the data loaded using DataLoader module.\n",
    "train_data = DataLoader(data['train'], batch_size=bs, shuffle=True)\n",
    "valid_data = DataLoader(data['valid'], batch_size=bs, shuffle=True)\n",
    "test_data = DataLoader(data['test'], batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Importing part of an existing model\n",
    "\n",
    "As with the Keras notebook, we load in a pre-trained model, and freeze the feature extraction layers so they cannot be retrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "# Make a model of your choice. Make sure you import the pre-trained model, and not just the architecture.\n",
    "# model = ...\n",
    "\n",
    "# Freeze model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Adding flattening and dense layers\n",
    "\n",
    "Now, we'll replace the the final layer with a concoction of our own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the final layer of model for Transfer Learning. Make sure you have only two output classes.\n",
    "model_inputs = model.classifier[1].in_features\n",
    "\n",
    "# model.classifier = nn.Sequential(\n",
    "#    nn.Linear(model_inputs, ...),\n",
    "#    nn.ReLU(),\n",
    "#    nn.Linear(..., ...),\n",
    "#    nn.LogSoftmax(dim=1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "# Create appropriate loss function and optimizer.\n",
    "# loss_func = nn....\n",
    "# optimizer = optim...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As PyTorch is sometimes a bit involved, the training and validation loop is given to you here for free:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "epochs = 6\n",
    "history = []\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Loss and accuracy within the epoch.\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Clean existing gradients.\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - compute outputs on input data using the model.\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss.\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "\n",
    "            # Backpropagate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute the total loss for the batch and add it to train_loss.\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Compute the accuracy.\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "            \n",
    "            # Convert correct_counts to float and then compute the mean.\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "            # Compute total accuracy in the whole batch and add to train_acc.\n",
    "            train_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "            print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
    "        \n",
    "        # Validation - No gradient tracking needed.\n",
    "        with torch.no_grad():\n",
    "            # Set to evaluation mode.\n",
    "            model.eval()\n",
    "\n",
    "            # Validation loop.\n",
    "            for j, (inputs, labels) in enumerate(valid_data_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass - compute outputs on input data using the model.\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Compute loss.\n",
    "                loss = loss_criterion(outputs, labels)\n",
    "                \n",
    "                # Compute the total loss for the batch and add it to valid_loss.\n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Calculate validation accuracy.\n",
    "                ret, predictions = torch.max(outputs.data, 1)\n",
    "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "                # Convert correct_counts to float and then compute the mean.\n",
    "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "                # Compute total accuracy in the whole batch and add to valid_acc.\n",
    "                valid_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "                print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "\n",
    "        # Find average training loss and training accuracy.\n",
    "        avg_train_loss = train_loss/train_data_size\n",
    "        avg_train_acc = train_acc/float(train_data_size)\n",
    "\n",
    "        # Find average training loss and training accuracy.\n",
    "        avg_valid_loss = valid_loss/valid_data_size\n",
    "        avg_valid_acc = valid_acc/float(valid_data_size)\n",
    "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "        epoch_end = time.time()\n",
    "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, nttValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've gotten a bit of history after training this, make some nice plots as you usually do to show the training and validation curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracies.\n",
    "\n",
    "# Plot the training and validation losses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's test our model one last time using the test set. Here's some helper code to make the testing easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict(model, test_image_name):\n",
    "    transform = image_transforms['test']\n",
    "    test_image = Image.open(test_image_name)\n",
    "    plt.imshow(test_image)\n",
    "    test_image_tensor = transform(test_image)\n",
    "    if torch.cuda.is_available():\n",
    "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224).cuda()\n",
    "    else:\n",
    "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Model outputs log probabilities.\n",
    "        out = model(test_image_tensor)\n",
    "        ps = torch.exp(out)\n",
    "        topk, topclass = ps.topk(1, dim=1)\n",
    "        idx_to_class = ['Not a hot dog!', 'Yum, hot dog!']\n",
    "        \n",
    "        print(\"Output class :  \", idx_to_class[topclass.cpu().numpy()[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did you do? How many hot dogs did you successfully identify? Aren't you glad you'll never mistake a banana for a hot dog again?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "56e8a80556da08c0faeec593c3b6c626e8afc1ec1fe0580f9add8c75686ed1e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
