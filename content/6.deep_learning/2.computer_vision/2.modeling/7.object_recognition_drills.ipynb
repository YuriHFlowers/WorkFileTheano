{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object recognition drills\n",
    "\n",
    "You've seen an example of a simple neural network in the [previous](./5.object_recognition_from_scratch_with_keras.ipynb) [chapters](./6.object_recognition_from_scratch_with_pytorch.ipynb), but can you make your own as well? Let's flex our neural skills and **extend** the model from the previous chapter.\n",
    "\n",
    "## Multiple classes\n",
    "\n",
    "The previous model was alright in separating cats from dogs, but what if we want to add on more classes? Extend the previous model by **adding more classes** from the Google [\"quick,draw!\" dataset](https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/numpy_bitmap). \n",
    "\n",
    "Choose 6 classes of your own, train and test your model as shown in the previous notebook. You might need to change the model structure a bit to deal with the new classes. Hint: choose yourself some visually distinct classes to make your life a bit easier.\n",
    "\n",
    "You can choose whether you want to use keras or pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend the model shown in the previous chapter to recognise 6 different classes\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "bicycle = np.load(\"../assets/bicycle.npy\")\n",
    "bracelet = np.load(\"../assets/bracelet.npy\")\n",
    "bridge = np.load(\"../assets/bridge.npy\")\n",
    "horse = np.load(\"../assets/horse.npy\")\n",
    "airplane = np.load(\"../assets/airplane.npy\")\n",
    "bear = np.load(\"../assets/bear.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178286, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.bar([0,5], [bicycle.shape[0], bracelet.shape[0], bridge.shape[0], horse.shape[0], airplane.shape[0], bear.shape[0]])\\nplt.title('dataset sizes')\\nplt.xticks([0,5], ['bicycle', 'bracelet', 'bridge', 'horse','airplane', 'bear'])\\nplt.ylabel('number of samples');\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot\n",
    "'''\n",
    "plt.bar([0,5], [bicycle.shape[0], bracelet.shape[0], bridge.shape[0], horse.shape[0], airplane.shape[0], bear.shape[0]])\n",
    "plt.title('dataset sizes')\n",
    "plt.xticks([0,5], ['bicycle', 'bracelet', 'bridge', 'horse','airplane', 'bear'])\n",
    "plt.ylabel('number of samples');\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20000 samples is a nice balance to have enough data to have a nice\n",
    "# accuracy, without training for too long\n",
    "max_samples = 20000\n",
    "preprocessed_bicycle = bicycle[:max_samples].reshape(-1,28,28)\n",
    "preprocessed_bracelet = bracelet[:max_samples].reshape(-1,28,28)\n",
    "preprocessed_bridge = bridge[:max_samples].reshape(-1,28,28)\n",
    "preprocessed_horse = horse[:max_samples].reshape(-1,28,28)\n",
    "preprocessed_airplane = airplane[:max_samples].reshape(-1,28,28)\n",
    "preprocessed_bear = bear[:max_samples].reshape(-1,28,28)\n",
    "\n",
    "# Normalizing\n",
    "preprocessed_bicycle = preprocessed_bicycle/255\n",
    "preprocessed_bracelet = preprocessed_bracelet/255\n",
    "preprocessed_bridge = preprocessed_bridge/255\n",
    "preprocessed_horse = preprocessed_horse/255\n",
    "preprocessed_airplane = preprocessed_airplane/255\n",
    "preprocessed_bear = preprocessed_bear/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_bicycle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.bar([0,1], [preprocessed_bicycle.shape[0], preprocessed_bracelet.shape[0], preprocessed_bridge.shape[0], preprocessed_horse.shape[0], preprocessed_airplane.shape[0], preprocessed_bear.shape[0]])\\nplt.title('dataset sizes')\\nplt.xticks([0,5], ['dogs', 'cats'])\\nplt.ylabel('number of samples');\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PLOT\n",
    "'''\n",
    "plt.bar([0,1], [preprocessed_bicycle.shape[0], preprocessed_bracelet.shape[0], preprocessed_bridge.shape[0], preprocessed_horse.shape[0], preprocessed_airplane.shape[0], preprocessed_bear.shape[0]])\n",
    "plt.title('dataset sizes')\n",
    "plt.xticks([0,5], ['dogs', 'cats'])\n",
    "plt.ylabel('number of samples');\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_labels = np.zeros((max_samples, 1))\n",
    "dog_labels = np.ones((max_samples, 1))\n",
    "\n",
    "labels = np.concatenate([cat_labels, dog_labels])\n",
    "drawings = np.concatenate([preprocessed_cats, preprocessed_dogs])\n",
    "\n",
    "\n",
    "# tensorflow wants a 4D tensor with (n_images, width, height, colour_depth)\n",
    "print(\"Drawings shape before : \", drawings.shape)\n",
    "drawings = np.expand_dims(drawings, axis=3)\n",
    "print(\"Drawings shape after : \", drawings.shape)\n",
    "print(\"Label shape : \", labels.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_drawings, test_drawings, train_val_labels, test_labels = train_test_split(\n",
    "    drawings, \n",
    "    labels,\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_drawings, val_drawings, train_labels, val_labels = train_test_split(\n",
    "    train_val_drawings, \n",
    "    train_val_labels,\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "print(\"train_drawings shape : \", train_drawings.shape)\n",
    "print(\"val_drawings shape : \", val_drawings.shape)\n",
    "print(\"test_drawings shape : \", test_drawings.shape)\n",
    "\n",
    "print(\"train_labels shape : \", train_labels.shape)\n",
    "print(\"val_labels shape : \", val_labels.shape)\n",
    "print(\"test_labels shape : \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full-color dataset\n",
    "\n",
    "Drawings are nice, but how good are CNN's for nice color pictures? Download [this](https://www.kaggle.com/moltean/fruits) dataset to get pictures of fruits and vegetables. Design (or use transfer learning) and evaluate a CNN to classify these (remember; these are RGB color pictures). \n",
    "\n",
    "Hint: take some inspiration from already successful and [popular CNN architectures](https://www.topbots.com/important-cnn-architectures/), from [keras built-in NN models](https://keras.io/api/applications/) or [pytorch built-in ones](https://pytorch.org/vision/0.8/models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design and evaluate a CNN for classifying fruits and vegetables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing feature extraction\n",
    "\n",
    "Alright, time for some nice visualizations! Neural networks are notoriously **hard to interpret**, and all their hidden variables make for a very **unobservable** transformation. How can we, as humans, still visualize our neural network, though?\n",
    "\n",
    "We can visualize the feature maps, which is what each convolutional layer 'sees' after the filters are applied. Follow the steps described in [this article](https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/) to try this out on your fruit CNN! (or one of the CNN's in the previous chapters in case you have not finished the previous drill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the feature maps of your fruit CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "56e8a80556da08c0faeec593c3b6c626e8afc1ec1fe0580f9add8c75686ed1e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
