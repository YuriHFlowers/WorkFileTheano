{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning\n",
    "\n",
    "In order to have an accurate result with your NLP model, you need to give all possible information you can to the model. (Only the ones that are useful and well-formatted, of course.)\n",
    "\n",
    "For example, if you have an image before each important word in a text, or some block of text separated by a lot of spaces.\n",
    "\n",
    "Let's take a concrete use-case:\n",
    "\n",
    "![text](https://i.imgur.com/2METpwn.png)\n",
    "\n",
    "In this image, you could provide the text like this:\n",
    "\n",
    "```\n",
    "Becode 1st December 2020 Cantersteen 10 Bruxelles 1000 Bruxelles Dear learners,\n",
    "```\n",
    "\n",
    "But it will be hard for your model to extract meaningful informations out of it. Even for you, if I give you this text it will not be easy.\n",
    "\n",
    "A first solution could be to format and sort it.\n",
    "\n",
    "```\n",
    "Becode\n",
    "Cantersteen 10\n",
    "1000 Bruxelles\n",
    "\n",
    "1st December 2020\n",
    "Bruxelles\n",
    "\n",
    "Dear learners,\n",
    "```\n",
    "\n",
    "A bit better, but it's still not perfect because the model doesn't understand your line breaks, it only understands text and spaces (which are a part of text, too). So we can add a tag. \n",
    "\n",
    "## Html tags\n",
    "\n",
    "![html](https://cdn.lynda.com/course/170427/170427-637363828865101045-16x9.jpg)\n",
    "\n",
    "As a convention, people often use the same tag as the following HTML tag: `<br>` which stands for **B**reak **L**line.\n",
    "\n",
    "So we can do something like:\n",
    "```html\n",
    "Becode\n",
    "Cantersteen 10\n",
    "1000 Bruxelles\n",
    "\n",
    "1st December 2020\n",
    "Bruxelles\n",
    "<br>\n",
    "Dear learners,\n",
    "```\n",
    "\n",
    "As you saw in previous [chapters](../../2.python/2.python_advanced/05.Scraping/2.beautifulsoup_advanced.ipynb), HTML tags will also be important when scraping data from the web.\n",
    "\n",
    "## Create your own tags\n",
    "\n",
    "Sometimes, you want to add visual information that is not in the text. It could be emojis, recurrent images at specific places in front of the text, etc...\n",
    "\n",
    "In those cases, you can create your own tags, but **be careful to only do that if**:\n",
    "1. You are sure that this information will help the model\n",
    "2. There is enough repetition of this tag to allow the model to understand the meaning of it.\n",
    "\n",
    "For example in our letter, we could specify to the model that the address and the date are on a different side of the page. We could. decide to add a tag `<LEFT_SECTION>` (it's just a choice made to call it like that). If I only give this document or add other document that doesn't contain this tag, the model will not understand the meaning of it! But if I give 100 documents like that with the same tag each time, the model could start to understand the link.\n",
    "\n",
    "```\n",
    "Becode\n",
    "Cantersteen 10\n",
    "1000 Bruxelles\n",
    "\n",
    "<LEFT_SECTION>\n",
    "1st December 2020\n",
    "Bruxelles\n",
    "</LEFT_SECTION>\n",
    "\n",
    "\n",
    "Dear learners,\n",
    "```\n",
    "\n",
    "## Tags are sometime dangerous\n",
    "\n",
    "**Pro tips:** Sometimes, you will find those tags in the extracted text, you should always ask yourself: *Does it make sense or is it confusing?*\n",
    "\n",
    "For example, in this text:\n",
    "\n",
    "```\n",
    "Lorem Ipsum is simply dummy text of the printing and typesetting industry.\n",
    "\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of\n",
    "\n",
    " type and scrambled it to make a type specimen book. \n",
    "```\n",
    "\n",
    "Here the line break doesn't add any information, it's more for the style and the readability.\n",
    "So if you extract those line breaks (and you will with some document formats), you get\n",
    "```\n",
    "Lorem Ipsum is simply dummy text of the printing and typesetting industry.<br>\n",
    "\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of<br>\n",
    "\n",
    " type and scrambled it to make a type specimen book. \n",
    "```\n",
    "\n",
    "You should remove them! You could, for example, use regular expressions for that.\n",
    "\n",
    "You will also encounter formatting tags like `<b>` or `<i>` (bold and italic). Once again, depending on your task, you may want to remove them. If you do document classification it can totally bias the model, if you do Named Entity Recognition (which we'll see in a later chapter), it could definitely help the model.\n",
    "\n",
    "Try to always ask yourself the question: *Would it help me to do the task or not?* If the answer is no, just remove them.\n",
    "\n",
    "## Practice time!\n",
    "\n",
    "Remove all the HTML tags in the text below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " WWF's mission is to stop the  degradation  of our planet's natural environment. \n"
     ]
    }
   ],
   "source": [
    "# import regular expression\n",
    "import re\n",
    "\n",
    "text = \"<p> WWF's mission is to stop the <strong> degradation </strong> of our planet's natural environment. </p>\"\n",
    "\n",
    "# Remove all HTML tags in 'text'\n",
    "no_tags = re.sub('<[^<]+?>', '',text)\n",
    "print(no_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lowercase the text and spaces\n",
    "\n",
    "## Text casing\n",
    "\n",
    "The text casing will also have an influence on your model. If you are looking for address, names, and so on, it will help the model. But if you want to do document classification, the model will do a difference between `doctor` and `Doctor` and you don't want that. One way to avoid this is by changing all the text to lowercase.\n",
    "\n",
    "## Space trailing\n",
    "\n",
    "When you extract text from documents, sometimes you will have additional spaces after a sentence or a double space where there shouldn't be one. These are just formatting errors, but your model will be affected.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    " This  is   some text where some spaces  have been added.  \n",
    "   Remove them! \n",
    "```\n",
    "\n",
    "In this text it would be easy to remove the spaces but you will not do it by hand for each documents!\n",
    "\n",
    "\n",
    "## Practice time!\n",
    "\n",
    "Remove all the double spaces and the single space at the start or end of the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting , remaining essentially unchanged . It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \" Lorem Ipsum is  simply dummy text  of the printing and typesetting industry. Lorem Ipsum has been  the industry's standard dummy text ever since the  1500s, when an unknown printer took a galley  of type and scrambled it to  make a type specimen  book. It has survived  not only five centuries, but  also the leap  into electronic typesetting ,  remaining essentially unchanged . It was popularised  in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop  publishing software like Aldus PageMaker including versions of Lorem Ipsum. \"\n",
    "#remove spaces from beginning of string\n",
    "text = text.lstrip() \n",
    "#remove spaces \n",
    "text = text.strip()\n",
    "text = text.rstrip()\n",
    "text = re.sub('\\s+', ' ', text)\n",
    "# Remove all the useless spaces in 'text'\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting , remaining essentially unchanged . It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" Lorem Ipsum is  simply dummy text  of the printing and typesetting industry. Lorem Ipsum has been  the industry's standard dummy text ever since the  1500s, when an unknown printer took a galley  of type and scrambled it to  make a type specimen  book. It has survived  not only five centuries, but  also the leap  into electronic typesetting ,  remaining essentially unchanged . It was popularised  in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop  publishing software like Aldus PageMaker including versions of Lorem Ipsum. \"\n",
    "#remove spaces from beginning of string\n",
    "' '.join(text.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words\n",
    "\n",
    "## What are stop words?\n",
    "\n",
    "A stop word is a word that has very little meaning by itself, such as `the`,`a`, `and`, `an`,...\n",
    "Most search engines remove these \"stop words\" when you do a search.\n",
    "\n",
    "![stop words](https://i2.wp.com/xpo6.com/wp-content/uploads/2009/04/stop-words.png?fit=837%2C499)\n",
    "\n",
    "## How to remove these stop words?\n",
    "\n",
    "You could remove them by hand with the `replace()` function, but if you want to go faster, you can use libraries like `SpaCy`, `NLTK`,  `Gensim`, and more. Each library will behave slightly differently, but not enough to make big changes to your model.\n",
    "\n",
    "## Practice time!\n",
    "\n",
    "Using the library of your choice, remove all the stop words of this text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note-yuri:  \n",
    "\n",
    "[1.Text pre-processing: Stop words removal using different libraries](https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a)\n",
    "\n",
    "[Removing Stop Words from Strings in Python](https://stackabuse.com/removing-stop-words-from-strings-in-python/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all my stop words\n",
    "text = \"At BeCode, we like to learn. Sometime, we play games not win a price but to have fun!\"\n",
    "\n",
    "# You can use any library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# import the library and the English stop words list:\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "sw_nltk = stopwords.words('english')\n",
    "print(sw_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "#Let us check how many stop words this library has\n",
    "print(len(sw_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BeCode,', 'like', 'learn.', 'Sometimes,', 'play', 'games', 'win', 'price', 'fun!']\n",
      "BeCode, like learn. Sometimes, play games win price fun!\n",
      "Old length:  86\n",
      "New length:  56\n"
     ]
    }
   ],
   "source": [
    "text = \"At BeCode, we like to learn. Sometimes, we play games not win a price but to have fun!\"\n",
    "words = [word for word in text.split() if word.lower() not in sw_nltk]\n",
    "print(words)\n",
    "new_text = \" \".join(words)\n",
    "print(new_text)\n",
    "print(\"Old length: \", len(text))\n",
    "print(\"New length: \", len(new_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should be something like this:\n",
    "```\n",
    "['BeCode', ',', 'like', 'learn', '.', ',', 'play', 'games', 'win', 'price', 'fun', '!']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'make', '’m', 'he', 'from', 'below', 'well', \"'ll\", '’re', 'even', 'hereafter', 'those', 'become', 'therefore', 'becomes', 'both', 'as', 'than', 'either', 'of', 'during', 'where', 'am', 'ourselves', 'their', 'sixty', 'ten', 'three', 'not', 'last', 'own', 'that', 'himself', 'moreover', 'became', 'we', 'somehow', 'hundred', 'top', 'cannot', 'may', 'keep', 'four', 'say', 'itself', 'whither', 'just', 'unless', 'made', 'ca', 'twenty', 'anything', 'less', 'another', 'everyone', 'besides', 'whether', 'all', 'has', 'using', 'afterwards', 'then', 'its', 'might', 'mine', 'six', '‘re', 'more', 'his', '‘ll', 'many', 'please', 'hence', 'among', 'one', 'hereby', 'along', 'upon', 'eight', 'onto', 'neither', 'two', 'third', 'regarding', 'within', 'wherever', 'sometimes', 'which', 'nor', 'here', 'for', 'hers', 'until', 'amongst', 'almost', 'least', 'so', 'whole', 'done', 'noone', 'though', 'elsewhere', 'because', 'used', 'without', 'indeed', 'whatever', 'whereby', 'except', 'again', 'via', 'namely', 'show', 'now', 'others', 'up', 'go', 'ever', 'who', '‘ve', 'nowhere', 'yourselves', '‘m', 'thence', \"'s\", '‘s', '’ve', 'behind', 'are', 'myself', 'move', 'there', 'n‘t', 'fifty', 'anyway', 'is', 'someone', 'see', 'whence', 'into', 'such', 'themselves', 'seem', 'yet', 'whom', 'thru', 'when', 'due', 'take', 'sometime', 'anyhow', 'forty', 'had', 'by', 'also', 'several', 'to', 'do', 'however', 'down', 'the', 'each', 'you', 'nine', 'be', 'give', 'doing', 'else', 'very', 'per', 'an', 'mostly', 'or', 'hereupon', 'every', \"'re\", 'anyone', 'eleven', 'beyond', 'she', 'quite', 'thereafter', 'put', 'whoever', 'still', 'five', 'toward', 'name', 'herein', 'but', 'together', 'first', 'across', 'perhaps', 'anywhere', 'our', 'really', 'him', 'meanwhile', 'can', 'with', 'most', 'why', 'back', 'been', 'a', 'everything', 'therein', 'beforehand', 'thereby', 'us', 'after', 'whereupon', 'should', 'nothing', 'get', 'something', 'other', 'since', 'herself', 'does', 'formerly', 'while', 'latter', 'on', 'side', 'once', 'seemed', 'too', 'everywhere', 'these', '‘d', 'often', 'same', 'ours', 'between', 'throughout', 'empty', 'no', 'thus', 'being', 'nevertheless', 'whereas', 'at', 'whenever', 'part', 'were', 'whereafter', '’ll', 'was', 'becoming', 'over', 'could', 'twelve', 'next', 'rather', 'various', 'amount', 'this', 'her', '’d', 'front', 'about', 'beside', 'serious', 'much', 'otherwise', 'in', 'towards', \"n't\", 'full', 'some', 'must', 'further', 'wherein', 'i', 'somewhere', 'few', 'they', 'my', 'nobody', 'latterly', 'them', 'yours', 'always', 'through', 'call', 'seems', 'before', 'any', 'what', 'yourself', 'if', 'seeming', 'it', 'already', 'never', 'under', 'fifteen', 'would', 'enough', 'did', 'me', 'around', 'former', 'none', 'n’t', \"'d\", 'alone', \"'m\", 'thereupon', 'whose', 'your', \"'ve\", 'although', 'against', 'off', 'above', 'out', 'have', 'only', 'how', 'bottom', 'will', 'and', 're', '’s'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "#loading the english language small model of spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "sw_spacy = en.Defaults.stop_words\n",
    "print(sw_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n"
     ]
    }
   ],
   "source": [
    "#Let us check how many stop words this library has\n",
    "print(len(sw_spacy))   # Notice, Spacy has more stop-words than NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BeCode,', 'like', 'learn.', 'Sometimes,', 'play', 'games', 'win', 'price', 'fun!']\n",
      "BeCode, like learn. Sometimes, play games win price fun!\n",
      "Old length:  86\n",
      "New length:  56\n"
     ]
    }
   ],
   "source": [
    "text = \"At BeCode, we like to learn. Sometimes, we play games not win a price but to have fun!\"\n",
    "\n",
    "words = [word for word in text.split() if word.lower() not in sw_spacy]\n",
    "print(words)\n",
    "new_text = \" \".join(words)\n",
    "print(new_text)\n",
    "print(\"Old length: \", len(text))\n",
    "print(\"New length: \", len(new_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should be something like this:\n",
    "```\n",
    "['BeCode', ',', 'like', 'learn', '.', ',', 'play', 'games', 'win', 'price', 'fun', '!']\n",
    "```\n",
    "\n",
    "So as you can see, depending on what kind of information you want to extract, you will be able to exclude stop words. For document classification or semantic search, you will not need those stop words for example.\n",
    "\n",
    "## Customize your stop words\n",
    "\n",
    "You can also add or remove stop words from the list that the libraries uses for stop words. If there is a specific word in your document that should not be considered as a stop word, or one that should absolutely be given to the model, you can do so.\n",
    "\n",
    "\n",
    "## Additional resources\n",
    "* [NLP Essentials: Removing stopwords and performing Text Normalization using NLTK and spaCy in Python](https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/)\n",
    "* [Removing stop words from strings in Python](https://stackabuse.com/removing-stop-words-from-strings-in-python/#usingpythonsnltklibrary)\n",
    "* [Dropping common terms: stop words](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('myNLTK')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6e93389139a38db427026b9637522739d6fe06c48619f3fde42a7ea95a9cf1c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
