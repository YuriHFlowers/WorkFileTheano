{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "\n",
    "## Why Embedding?\n",
    "As we know, machines can't handle text, it can only handle numbers. But how to convert a word to numbers?\n",
    "\n",
    "The most naive approach would be to take a list of all the words in your text and attribute a number to all of them. It will work but you can imagine that some problems will appear:\n",
    "* How do you handle unknown words? \n",
    "* If your text contains `doctor`, `nurse`, and `candy`. `doctor` and `nurse` have a strong similarity but `candy` doesn't. How can we make the machine understand that? With our naive technique, `doctor` could have the number `5` associated to it and nurse the number `98767`.\n",
    "\n",
    "Of course, a lot of people already spent some time with those problems. the solution that came out of it is \"Embedding\". \n",
    "\n",
    "## What is embeddings?\n",
    "\n",
    "An embedding is a **VECTOR** which represents a word or a document.\n",
    "\n",
    "A vector will be attributed to each token. Each vector will contain multiple dimensions (usually tens or hundreds of dimensions).\n",
    "\n",
    "```\n",
    "[...] associate with each word in the vocabulary a distributed word feature vector [...] The feature vector represents different aspects of the word: each word is associated with a point in a vector space. The number of features [...] is much smaller than the size of the vocabulary.\n",
    "```\n",
    "- [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf), 2003.\n",
    "\n",
    "Long story short, embeddings convert words into vectors in a way that allows the machine to understand the similarity betweens them.\n",
    "\n",
    "Each embedding library has it's own way of classifying words, it will regroup words into big categories. Each word will get a score for each category.\n",
    "\n",
    "To take a simple example the word `mother` could be classified like that:\n",
    "\n",
    "|        | female | family | human | animal|\n",
    "|--------|--------|---------|-------|-------|\n",
    "| mother | 0.9    | 0.9.    | 0.7   | 0.1   |\n",
    "\n",
    "**Explanations:** Mother has a strong similarity with female, family and human but it has a low similarity with animal.\n",
    "\n",
    "**Disclaimer:** Those numbers and categories are totally arbitrary and are only here to show an example.\n",
    "\n",
    "Here is another example with more complete datas:\n",
    "\n",
    "![embedding](https://miro.medium.com/max/2598/1*sAJdxEsDjsPMioHyzlN3_A.png)\n",
    "\n",
    "## Should I do it by hand?\n",
    "\n",
    "You could, but if some people already did the job for you and spent a lot of time to optimize it, why not use it?\n",
    "\n",
    "## What to use?\n",
    "\n",
    "There are a lot of libraries out there for embeddings. Which one is the best? Once again, *it depends*. The results will change depending on the text you are using, the information you want to extract, the model you use,...\n",
    "\n",
    "Choosing the \"best\" embedding model will be part of the hyper-optimization that you can do at the end of a project.\n",
    "\n",
    "If you want understand embeddings more in depth, [follow this link](http://jalammar.github.io/illustrated-word2vec/)\n",
    "\n",
    "Here are some of the best libraries of the moment:\n",
    "\n",
    "* [Flair](https://github.com/flairNLP/flair) (University of Berlin)\n",
    "* [fasttext](https://fasttext.cc/) (Facebook)\n",
    "* [GloVe](https://github.com/stanfordnlp/GloVe) (Stanford)\n",
    "\n",
    "And the oldest way doing it (but still good):\n",
    "* [Word2Vec](https://www.tensorflow.org/tutorials/text/word2vec)\n",
    "\n",
    "## Stack embeddings\n",
    "\n",
    "The previous embeddings are good, but if you want something even better, you can \"stack\" these embeddings to create a bigger vector. It gives better results but will also require more computation power.\n",
    "\n",
    "[Here is a super clear and understandable guide](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md) to get it done. (by the Flair's team)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice time!\n",
    "\n",
    "Enough reading, let's practice a bit. Can you use SpaCy to embed this sentence?\n",
    "Read the [spacy embedding documentation](https://spacy.io/usage/vectors-similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I love learning\"\n",
    "\n",
    "# Embed `sentence` with SpaCy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I True 8.277152 True\n",
      "love True 7.836812 True\n",
      "learning True 8.697989 True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokens = nlp(sentence)\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I [ 1.10176355e-01 -1.00206280e+00 -7.37348914e-01 -4.21369344e-01\n",
      "  1.26815522e+00  1.01282895e-01  1.22158146e+00  9.41173211e-02\n",
      "  6.11733913e-01  2.67086446e-01 -5.15017867e-01  9.33182955e-01\n",
      "  1.34804058e+00  2.07552522e-01 -1.10650647e+00  2.18127221e-01\n",
      " -8.12490225e-01 -3.19170117e-01  6.82181120e-01 -6.72757030e-02\n",
      "  6.34525537e-01 -1.26337200e-01 -2.08082974e-01 -1.03394032e+00\n",
      "  2.46634698e+00  6.73271194e-02 -1.00889707e+00  2.34798193e-02\n",
      " -1.18598306e+00  6.71017766e-02 -6.87737346e-01 -2.94678241e-01\n",
      " -1.80477571e+00 -6.30941615e-02 -4.73648787e-01 -1.43817735e+00\n",
      " -9.56059933e-01  1.78785622e-01 -1.27222240e-01  8.63844812e-01\n",
      "  1.01742697e+00  1.56876707e+00 -4.96611297e-01  4.25037891e-01\n",
      " -2.50088990e-01  1.40886888e-01 -7.53663301e-01 -5.81104994e-01\n",
      "  9.16368961e-02 -9.78723586e-01  1.15381956e-01 -3.34213763e-01\n",
      "  1.12113702e+00  2.86907703e-01  9.25659895e-01  3.18265676e+00\n",
      " -1.24204904e-03  2.87270963e-01  5.06109953e-01  3.47291678e-01\n",
      "  6.93372667e-01 -6.36061788e-01 -1.44981816e-01 -1.32679880e-01\n",
      " -5.51123977e-01  8.23874474e-01 -1.30275416e+00 -1.16080940e-01\n",
      "  7.36323833e-01 -1.02015269e+00 -1.76402152e-01 -2.92736620e-01\n",
      " -1.76140809e+00 -5.44232607e-01 -1.55945271e-01  6.57843411e-01\n",
      " -1.12985718e+00 -9.35547054e-01  8.30989778e-01 -6.64355278e-01\n",
      " -6.95689201e-01 -7.24484801e-01 -1.04905337e-01  1.49113166e+00\n",
      "  6.86134934e-01 -7.70698488e-01  1.65820301e-01  1.45997715e+00\n",
      " -3.52823168e-01 -6.64176822e-01 -3.35974813e-01  5.24557114e-01\n",
      "  3.28140110e-01  9.47857440e-01 -3.98438901e-01 -9.41711366e-02]\n",
      "love [-0.06299812 -0.22825542  0.0824195   0.05026802  0.3636278   0.18694513\n",
      "  0.65269005  0.53847086  1.2072396   0.6055027   0.595559   -0.20271295\n",
      " -0.8922394  -0.7060792  -1.0196091  -1.206897    0.30723915  1.1273683\n",
      " -1.0133196   1.511328    1.9707104  -0.4499414  -0.30668157 -0.3416161\n",
      " -0.6900256  -0.04276395 -0.24816895 -0.2702095   0.15748334  0.33143553\n",
      "  0.72608936  1.3908917   0.45960402  0.07220685 -0.4348278  -1.2088721\n",
      "  0.45895672  0.21447465  1.7094007   0.20196046  0.4213842   1.2660861\n",
      " -0.15651977 -1.3901308  -0.6193261  -0.50554365 -1.0523047  -0.6644855\n",
      " -0.7948754   0.67246926 -0.7394111   0.33138514  1.4490907   1.7405348\n",
      " -0.04477307 -0.6761043  -1.1175388  -0.25067604  0.1201548  -0.6791054\n",
      " -0.21771799 -1.1226994  -0.37164557 -0.9776875  -0.41967183 -0.7941864\n",
      " -0.46075112 -0.94622123 -0.29932624 -1.8960687   0.8826493   1.2701998\n",
      " -0.94835824  0.04121277  0.8318188   0.7182481   0.19510043 -1.2258778\n",
      " -0.46312594 -0.08690477 -0.40408614 -0.78081316  0.8175198   0.26752183\n",
      "  0.88382804 -0.41809648 -0.5920239   0.20349811  1.2233176  -1.0437187\n",
      " -0.45960334 -0.12778154  0.37355685  1.1663226   0.01148911 -0.84057605]\n",
      "learning [-4.8956251e-01  3.2296130e-01  7.7606052e-01  1.7376308e+00\n",
      "  3.6134070e-01  9.8890901e-02 -1.9538322e-01 -3.9373612e-01\n",
      " -7.5106934e-02 -1.2111921e+00 -1.4538807e-01  1.7389140e-01\n",
      " -3.2801265e-01 -1.2524943e-01 -4.0175620e-01 -5.4232681e-01\n",
      " -3.7267265e-01 -1.0514063e-01 -1.2160295e+00  8.1455249e-01\n",
      "  2.2081325e+00 -1.0273969e+00 -6.5043503e-01 -1.2592324e+00\n",
      " -1.3062525e+00 -6.4719760e-01 -8.5611719e-01  4.8151812e-01\n",
      "  1.7273731e+00 -7.1695948e-01  7.7802271e-01 -2.6471436e-02\n",
      "  1.4203355e+00 -2.7863196e-01 -2.9901743e-01 -9.9587351e-01\n",
      "  8.5439271e-01 -7.8055620e-01  1.7301289e+00 -5.6871831e-01\n",
      "  6.1158359e-01  1.6392478e-01  1.8968177e+00 -6.4175892e-01\n",
      "  8.0750471e-01  8.2304901e-01 -5.9138608e-01  2.2004828e-01\n",
      " -2.7646339e-01  2.1158462e+00 -3.0828768e-01  3.0213505e-01\n",
      " -4.8622715e-01  4.5745903e-01  1.8791277e+00  9.9251425e-01\n",
      " -1.2967995e+00 -3.3789408e-01 -2.8669256e-01 -9.4120115e-01\n",
      "  1.3682950e-01 -1.3737835e+00 -3.1728244e-01  1.0006949e+00\n",
      " -3.4467182e-01 -9.7382128e-01 -5.5328511e-02 -5.5818802e-01\n",
      " -6.3866997e-01 -8.8659918e-01  4.4279522e-01 -4.3326268e-01\n",
      "  4.0374100e-03 -7.9290414e-01 -1.1335506e+00 -1.7106764e-01\n",
      "  9.5313042e-04 -1.2543950e+00  1.2921427e-01  1.5239273e+00\n",
      "  3.2148260e-01 -1.4084818e+00  1.6142962e+00 -1.0499114e-01\n",
      "  3.7333179e-02 -3.0935663e-01 -8.5560685e-01 -1.6497719e-01\n",
      "  2.1478202e+00  6.3598394e-01 -3.1435138e-01  8.4164792e-01\n",
      " -1.0709527e+00 -5.4583800e-01  8.2395363e-01 -2.4275182e-01]\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token.text, token.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the shape of each word's vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96,)\n",
      "(96,)\n",
      "(96,)\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token.vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I tried this is not part of the material\n",
    "from spacy.vectors import Vectors\n",
    "import numpy as np\n",
    "empty_vectors = Vectors(shape=(10000, 300))\n",
    "\n",
    "data = np.zeros((3, 300), dtype='f')\n",
    "keys = [\"I\", \"love\", \"learning\"]\n",
    "vectors = Vectors(data=data, keys=keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.vectors.Vectors at 0x294642361c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the shape of each word's vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u'I').vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with Flair now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I love learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yurit\\anaconda3\\envs\\myFlair\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Sentence: \"I love learning\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.embeddings import FlairEmbeddings\n",
    "from flair.data import Sentence\n",
    "\n",
    "# init embedding\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "\n",
    "# create a sentence\n",
    "sentence = Sentence(\"I love learning\")\n",
    "\n",
    "# embed words in sentence\n",
    "flair_embedding_forward.embed(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"I\"\n",
      "tensor([ 0.0040,  0.0002,  0.0748,  ..., -0.0040,  0.0220,  0.0052],\n",
      "       device='cuda:0')\n",
      "torch.Size([2048])\n",
      "Token[1]: \"love\"\n",
      "tensor([-2.8628e-04, -8.3362e-05,  2.4467e-03,  ..., -3.9199e-03,\n",
      "        -1.2964e-02,  4.7443e-03], device='cuda:0')\n",
      "torch.Size([2048])\n",
      "Token[2]: \"learning\"\n",
      "tensor([-0.0001,  0.0002,  0.0244,  ..., -0.0041, -0.0057,  0.0073],\n",
      "       device='cuda:0')\n",
      "torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)\n",
    "    print(token.embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the shape of each word's vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended Flair Usage\n",
    "\n",
    "We recommend combining both forward and backward Flair embeddings. Depending on the task, we also recommend adding standard word embeddings into the mix. So, our recommended StackedEmbedding for most English tasks is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings\n",
    "\n",
    "# create a StackedEmbedding object that combines glove and forward/backward flair embeddings\n",
    "stacked_embeddings = StackedEmbeddings([\n",
    "                                        WordEmbeddings('glove'),\n",
    "                                        FlairEmbeddings('news-forward'),\n",
    "                                        FlairEmbeddings('news-backward'),\n",
    "                                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"I\"\n",
      "tensor([-0.0465,  0.6197,  0.5665,  ..., -0.0010, -0.0152, -0.0501],\n",
      "       device='cuda:0')\n",
      "Token[1]: \"love\"\n",
      "tensor([ 0.2598,  0.5583,  0.5799,  ...,  0.0219, -0.0060, -0.0601],\n",
      "       device='cuda:0')\n",
      "Token[2]: \"learning\"\n",
      "tensor([ 0.6481,  0.6988, -0.3995,  ...,  0.0016, -0.0222, -0.0022],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence('I love learning')\n",
    "\n",
    "# just embed a sentence using the StackedEmbedding as you would with any single embedding.\n",
    "stacked_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the shape of each word's vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"I\"\n",
      "torch.Size([4196])\n",
      "Token[1]: \"love\"\n",
      "torch.Size([4196])\n",
      "Token[2]: \"learning\"\n",
      "torch.Size([4196])\n"
     ]
    }
   ],
   "source": [
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same with Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I love learning\"\n",
    "\n",
    "\n",
    "# Embed `sentence` with Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the shape of each word's vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I love learning\"\n",
    "\n",
    "\n",
    "# Embed `sentence` with glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the shape of each word's vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your text is now embedded, your model will be able to understand it, yeah!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maths on text\n",
    "\n",
    "Since the words are embedded into vectors we can now apply mathematical methods on them.\n",
    "\n",
    "### Average vector\n",
    "\n",
    "For example we could build the average vector for a text by using NumPy! This is a straightforward way to build one single representation for a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I want to be a famous data scientist\"\n",
    "\n",
    "# Apply a spacy model on the text\n",
    "\n",
    "# Get all word vectors into a list\n",
    "\n",
    "# Compute and display the average vector of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'want', 'to', 'be', 'a', 'famous', 'data', 'scientist']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.30562106, -0.46668267,  0.18118   ,  0.10908538,  0.41245294,\n",
       "        0.03734358,  0.32639813, -0.13535896,  0.09729309, -0.05765083,\n",
       "        0.2214193 , -0.04172044,  0.26377854,  0.49861214, -0.17263839,\n",
       "       -0.15902124, -0.20183812,  0.03181725, -0.57855135,  0.04326862,\n",
       "        0.907686  , -0.08480154,  0.12812576, -0.20969304,  0.4940275 ,\n",
       "        0.03705223, -0.06355449,  0.14762354, -0.39961818, -0.27418312,\n",
       "       -0.38811898,  0.05215233,  0.0750602 , -0.18713821, -0.53578055,\n",
       "        0.22827083, -0.18803607, -0.19935909,  0.1113503 , -0.1084053 ,\n",
       "        0.28453946,  0.1037382 , -0.48079926, -0.10080262,  0.16023256,\n",
       "       -0.02597305,  0.41315472, -0.8075175 , -0.18440957,  0.03429174,\n",
       "       -0.18987596, -0.2646015 , -0.08954486,  0.24843045,  0.00751189,\n",
       "        0.34270817,  0.10769716,  0.07470732,  0.3089543 , -0.0607796 ,\n",
       "       -0.09832704, -0.569713  , -0.2613706 ,  0.37682348, -0.17064035,\n",
       "       -0.08847697,  0.00259911, -0.14756161, -0.42058748, -0.4235825 ,\n",
       "        0.11741035,  0.12185436,  0.18864046,  0.40212658, -0.41592836,\n",
       "        0.21418828,  0.4933724 ,  0.26732028, -0.11135626,  0.1493004 ,\n",
       "       -0.2846952 , -0.5375592 ,  0.20711276,  0.5742195 , -0.26853675,\n",
       "        0.2733441 ,  0.07764968, -0.05755274,  0.22427383, -0.06282802,\n",
       "        0.05021307, -0.14922318,  0.23223326,  0.13899206, -0.13046432,\n",
       "       -0.11280645], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "text = (\"I want to be a famous data scientist\")\n",
    "# Apply a spacy model on the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract tokens for the given doc\n",
    "print ([token.text for token in doc])\n",
    "\n",
    "# Compute and display the average vector of the text\n",
    "doc.vector   # Average vector of the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30562106, -0.46668267,  0.18118   ,  0.10908538,  0.41245294,\n",
       "        0.03734358,  0.32639813, -0.13535896,  0.09729309, -0.05765083,\n",
       "        0.2214193 , -0.04172044,  0.26377854,  0.49861214, -0.17263839,\n",
       "       -0.15902124, -0.20183812,  0.03181725, -0.57855135,  0.04326862,\n",
       "        0.907686  , -0.08480154,  0.12812576, -0.20969304,  0.4940275 ,\n",
       "        0.03705223, -0.06355449,  0.14762354, -0.39961818, -0.27418312,\n",
       "       -0.38811898,  0.05215233,  0.0750602 , -0.18713821, -0.53578055,\n",
       "        0.22827083, -0.18803607, -0.19935909,  0.1113503 , -0.1084053 ,\n",
       "        0.28453946,  0.1037382 , -0.48079926, -0.10080262,  0.16023256,\n",
       "       -0.02597305,  0.41315472, -0.8075175 , -0.18440957,  0.03429174,\n",
       "       -0.18987596, -0.2646015 , -0.08954486,  0.24843045,  0.00751189,\n",
       "        0.34270817,  0.10769716,  0.07470732,  0.3089543 , -0.0607796 ,\n",
       "       -0.09832704, -0.569713  , -0.2613706 ,  0.37682348, -0.17064035,\n",
       "       -0.08847697,  0.00259911, -0.14756161, -0.42058748, -0.4235825 ,\n",
       "        0.11741035,  0.12185436,  0.18864046,  0.40212658, -0.41592836,\n",
       "        0.21418828,  0.4933724 ,  0.26732028, -0.11135626,  0.1493004 ,\n",
       "       -0.2846952 , -0.5375592 ,  0.20711276,  0.5742195 , -0.26853675,\n",
       "        0.2733441 ,  0.07764968, -0.05755274,  0.22427383, -0.06282802,\n",
       "        0.05021307, -0.14922318,  0.23223326,  0.13899206, -0.13046432,\n",
       "       -0.11280645], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "text = \"I want to be a famous data scientist\"\n",
    "\n",
    "# Apply a spacy model on the text\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # or \"en_core_web_lg\"\n",
    "\n",
    "# Get all word vectors into a list  \n",
    "doc = nlp(text)\n",
    "\n",
    "# Compute and display the average vector of the text\n",
    "doc.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word similarity\n",
    "\n",
    "We can also compute the similarity between two words by using distance measures (e.g. [cosine distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html), [euclidean distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html)...). These measures will calculate the distance between word embeddings in the vector space.\n",
    "\n",
    "#### Let's practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4019054174423218"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from scipy.spatial import distance\n",
    "\n",
    "word1 = \"computer\"\n",
    "word2 = \"keyboard\"\n",
    "\n",
    "# Get the vector for both words through your favorite model\n",
    "w1 = nlp(word1).vector\n",
    "w2 = nlp(word2).vector\n",
    "\n",
    "# Compute the cosine and the euclidean distance between both words\n",
    "distance.cosine(w1, w2)\n",
    "\n",
    "# Try with other pairs of words for comparing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.418715000152588"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.euclidean(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5425749392020239"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think is better use Similarity\n",
    "# Import the required libraries\n",
    "import spacy\n",
    "nlp_lg = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "word1 = \"computer\"\n",
    "word2 = \"keyboard\"\n",
    "\n",
    "# Get the vector for both words through your favorite model\n",
    "word1_vector = nlp_lg(word1)\n",
    "word2_vector = nlp_lg(word2)\n",
    "\n",
    "# Compute the cosine and the euclidean distance between both words\n",
    "word1_vector.similarity(word2_vector)\n",
    "\n",
    "# Try with other pairs of words for comparing the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8416876491960881"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1 = \"brave\"\n",
    "word2 = \"courageous\"\n",
    "\n",
    "word1_vector = nlp_lg(word1)\n",
    "word2_vector = nlp_lg(word2)\n",
    "\n",
    "word1_vector.similarity(word2_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More resources\n",
    "* [Why do we use word embeddings in NLP?](https://towardsdatascience.com/why-do-we-use-embeddings-in-nlp-2f20e1b632d2)\n",
    "* [More details on what word embeddings are exactly?](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('myNLPspacy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "ecfcd950ad5b356582e6956082f14967dd485a1667f1520de802280b6ade099a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
