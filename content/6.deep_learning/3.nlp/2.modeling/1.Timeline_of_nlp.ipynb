{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Og9SFHPSefpf"
   },
   "source": [
    "# Timeline of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3_aAVkCefpl"
   },
   "source": [
    "## Symbolic NLP (1950s - 1990s)\n",
    "\n",
    "There are two main dates during this period.  \n",
    "\n",
    "In 1950 **[IBM created the first model capable of translating](https://en.wikipedia.org/wiki/Georgetown%E2%80%93IBM_experiment)** 60 sentences from Russian into English. This was the first approach to machine translation. Unfortunately, the algorithm was only able to translate these 60 sentences! As soon as an unknown sentence was presented to the model, it was no longer able to translate it.\n",
    "\n",
    "**[Eliza](https://en.wikipedia.org/wiki/ELIZA)** is a computer program written by Joseph Weizenbaum between 1964 and 1966, which simulates a psychotherapist by reformulating most of the \"patient's\" assertions into questions and asking them. Although Eliza is considered the first chatbot, this model had no level of comprehension, it really just rephrased the questions.\n",
    "\n",
    "Throughout this period, a rules-based symbolic approach was used. Rules-Based uses Linguistic rules and patterns. *E.g English has the structure of SVO (Subject Verb Object), Hindi has SOV (Subject Object Verb)*.  This has been made possible by regular expressions and [context-free grammar](https://en.wikipedia.org/wiki/Context-free_grammar).\n",
    "\n",
    "\n",
    "\n",
    "![](http://wiki.penson.io/images/cfg.png)\n",
    "\n",
    "[source](http://wiki.penson.io/images/cfg.png)\n",
    "\n",
    "The code could look like the part below, for a translation from English to Hindi.\n",
    "\n",
    "```\n",
    "\"have\" :=\n",
    "\n",
    "if \n",
    "  subject(animate)\n",
    "  and object(owned-by-subject)\n",
    "then \n",
    "  translate to \"Kade... aahe\"\n",
    "if \n",
    "  subject(animate)\n",
    "  and object(kinship-with-subject)\n",
    "then\n",
    "  translate to \"laa... aahe\"\n",
    "if \n",
    "  subject(inanimate)\n",
    "then \n",
    "  translate to \"madhye... aahe\"\n",
    "\n",
    "```\n",
    "\n",
    "But this approach has limits. Not only does one have to be an expert in linguistics, but one also has to create a considerable number of rules for moderate effectiveness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMgFPfuZfefS"
   },
   "source": [
    "| Advantages | Disadvantages|\n",
    "|------------|--------------|\n",
    "|No training time.|  Impossible to write all the rules because of the complexity of the languages. |\n",
    "| Quick to execute. | Impossibility to process unknown data. |\n",
    "| Efficient if the inputs are all known. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jkrANchqb3Q"
   },
   "source": [
    "**Exercise :** Explain using your own words (no copy and paste) what a symbolic approach is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQXet-yIqvRS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3hnPUY1efpm"
   },
   "source": [
    "## Statistical NLP (1990s - 2010s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXem55q5WvCN"
   },
   "source": [
    "The idea of the statistical approach is of course to find a model that can make generalities about unknown sentences rather than coding a whole bunch of rules that would be \"hardcoded\". \n",
    " \n",
    "The idea is based on a parallel corpus. It is given as input a sentence in English for example, and the model gives as output the translated sentence. \n",
    "\n",
    "We first tried to do word by word, it didn't work very well, because of the grammatical complexity of some languages. Then we created what we call n-grams. These are groups of words, unigram for one word, bigram for two words, trigram for three words and so on... So instead of doing word by word translations, we do them by groups of words. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFRJjtbAe2Uq"
   },
   "source": [
    "![](https://i.stack.imgur.com/8ARA1.png)\n",
    "\n",
    "[source](https://stackoverflow.com/questions/18193253/what-exactly-is-an-n-gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJ_tcM1i5IPX"
   },
   "source": [
    "During this period, there are 2 models that stand out for NLP : \n",
    "\n",
    "- [Logistic regression](https://en.wikipedia.org/wiki/Support-vector_machine)\n",
    "- [SVM](https://en.wikipedia.org/wiki/Support-vector_machine)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhUtbDcqzSj4"
   },
   "source": [
    "**Exercise :** Load a *sentiment analysis* dataset and create a logistic regression model for classification. (1= positive , 0 =negative). For example, you could use the `twitter_samples` dataset from the `nltk` library.\n",
    "\n",
    "* Tips : Use `CountVectorizer()` and `LogisticRegression()` from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuAeZqehI79R"
   },
   "outputs": [],
   "source": [
    "# MAX 10 LINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing some required libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yurit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\yurit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the stopwords for the process_tweet function\n",
    "nltk.download('stopwords')\n",
    "# downloads sample twitter dataset.\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Data   \n",
    "  The twitter_samples contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "twitter_samples.fileids()\n",
    "\n",
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# concatenate the lists, 1st part is the positive tweets followed by the negative\n",
    "tweets = all_positive_tweets + all_negative_tweets\n",
    "\n",
    "# convert to DataFrame\n",
    "df1 = pd.DataFrame (tweets, columns = ['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# make a numpy array representing labels of the tweets\n",
    "labels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))\n",
    "df2 = pd.DataFrame(labels)  # convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate \n",
    "df = pd.concat([df1, df2], axis=1, join='inner')\n",
    "df.rename(columns = {0:'sentiment'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='sentiment', ylabel='count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "# plotting the data using countplot\n",
    "sns.countplot(x=\"sentiment\",data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_cleaner(tweet):\n",
    "    # removing the urls from the text\n",
    "    tweet = re.sub(r'((www.\\S+)|(https?://\\S+))', r\"\", tweet)\n",
    "    #removing the numbers from the text\n",
    "    tweet = re.sub(r'[0-9]\\S+', r'', tweet)\n",
    "    #removing the tags from the text\n",
    "    tweet = re.sub(r'(@\\S+) | (#\\S+)', r'', tweet)\n",
    "    # removing the punctuation from the text\n",
    "    tweet_without_punctuation = [char for char in tweet if char not   \n",
    "                                in string.punctuation]\n",
    "    # converting the list to string \n",
    "    tweet_without_punctuation = \"\".join(tweet_without_punctuation) \n",
    "    # set of stop words \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    # removing the stop words \n",
    "    tweet_without_stopwords = [word for word in  \n",
    "                              tweet_without_punctuation.split()\n",
    "                              if word.lower() not in stop_words]\n",
    "    return tweet_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating input feature and label\n",
    "X=df.tweet\n",
    "y=df.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the wordcloud of the positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = df['tweet'][:5000]\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "               collocations=False).generate(\" \".join(map(str, data_pos)))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the wordcloud of the negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_neg = df['tweet'][5001:]\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "               collocations=False).generate(\" \".join(map(str, data_neg)))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting our data into Train and Test Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.20, random_state =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appliyng Countvectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 14028)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the features using count vectorizer\n",
    "vectorizer = CountVectorizer(analyzer = tweets_cleaner, dtype =     \n",
    "                            'uint8')\n",
    "df_countvectorizer = vectorizer.fit_transform(df['tweet'])\n",
    "df_countvectorizer.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer vs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_countvectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the features into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(  \n",
    "                                                df_countvectorizer,  \n",
    "                                                df[\"sentiment\"],   \n",
    "                                                test_size=0.2,  \n",
    "                                                 random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_class = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7605"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.80      0.77      1012\n",
      "         1.0       0.78      0.72      0.75       988\n",
      "\n",
      "    accuracy                           0.76      2000\n",
      "   macro avg       0.76      0.76      0.76      2000\n",
      "weighted avg       0.76      0.76      0.76      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model accuracy\n",
    "print(classification_report(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctoAelW8JpQY"
   },
   "source": [
    "OK, it works well! As you can see, the computer needs to translate the words into a vector to understand a sentence. But it's not that simple. In fact, for a computer, it is a very complex task. Indeed, the problem is quite simple.\n",
    "\n",
    "When you teach a model to recognize a cat, for example, the model actually learns to create a vector which is a universal representation of the cat. This is possible if you show him enough examples of cats !\n",
    "\n",
    "But for language, it is more complicated!\n",
    "\n",
    "There are so many ways to express oneself, so many subtleties in our languages, that it becomes very difficult for a computer to really understand the deep meaning of a sentence.\n",
    "\n",
    "Let's look at this sentence :\n",
    "\n",
    "<center><b>I do not recommend this product which is bad.</b></center>\n",
    "and : \n",
    "<center><b>I do recommend this product which is not bad.</b></center>  \n",
    "\n",
    "\n",
    "These two sentences contain the same words, but their meanings are different.\n",
    "\n",
    "The machine learning model will not be able to tell the difference between these two sentences.  With this model we lose an important piece of information, which is temporal information. Indeed, here, the order of the words has no effect on the prediction made by the model. However, as we have seen, this information can change the deep meaning of the sentence. \n",
    "\n",
    "Another problem, if we have a very large dataset, the computation time for training the model could be very long.\n",
    "\n",
    "For these reasons, we have started to study possible alternatives. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wA6Uw1OmhRBM"
   },
   "source": [
    "| Advantages | Disadvantages|\n",
    "|------------|--------------|\n",
    "| Can make a probalility when it receives an unknown input.|  The calculation time is long.  |\n",
    "| Allows to make generalities in order to avoid coding rules. | Loss of information (Word order)  |\n",
    "| | No context |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFKmUpyxefpm"
   },
   "source": [
    "## Neural NLP (2010s - present)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNZJY6wRpWB7"
   },
   "source": [
    "### Word2Vec  (2010 - 2014)\n",
    "\n",
    "Word2Vec is a module released in the 2010's. It's one of the first to use neural networks to make word representations. It's based on 2 architectures, CBOW and Skip-Gram. In both cases, they are 2-layer neural networks.\n",
    "\n",
    "CBOW uses surrounding words to predict who is in the middle. Skip-gram is used to understand the context of the sentence. It thus makes it possible, among other things, to make classification. \n",
    "\n",
    "![word2vec](https://miro.medium.com/max/2400/1*cuOmGT7NevP9oJFJfVpRKA.png)\n",
    "\n",
    "But we still have a problem. Certainly the model formation will be faster and more efficient, but we still don't have time information. He can recognize that the word \"apple\" is close to the word \"pear\". That's good enough! But the word order is still not taken into account in our model. He won't be able to differentiate the 2 sentences.\n",
    "\n",
    "<center><b>I do not recommend this product which is bad.</b></center>\n",
    "and : \n",
    "<center><b>I do recommend this product which is not bad.</b></center>  \n",
    "\n",
    "To try to bring a solution to this problem, we tried to work with recurrent neural networks. \n",
    "\n",
    "| Advantages | Disadvantages|\n",
    "|------------|--------------|\n",
    "| Can output a probability when it receives an unknown input.|  The calculation time is long.  |\n",
    "| Allows to make generalities in order to avoid coding rules. | Loss of information (Word order)  |\n",
    "| Can know the similarity between two words. (ex: 'Car, Motorcycle') | |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QexsVFH2sIWE"
   },
   "source": [
    "**Exercise :** Explain using your own words (no copy and paste) what a skip-gram is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQmI6XMuZZCo"
   },
   "source": [
    "### Recursive Neural Networks: RNN/LSTM (2014 - 2017)\n",
    "\n",
    "Recursive neural networks are similar to \"classical\" neural networks, but they differ from them in that they use feedback loops to process a sequence of data that shapes the final result. The end result may itself be a sequence of data. These feedback loops allow information to persist, an effect often equated with memory.\n",
    "\n",
    "All inputs are connected to each other and feed information back into the network. Put simply, the previous weights of a word can be changed by the following words. And the following words depend on the weights of the previous words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2G0GXXKefpo"
   },
   "source": [
    "So if we take the phrase \"what time is it?\" the vector of the word \"time\" will contain information about the previous word \"what\". Likewise, the vector of the word \"is\" will contain information of \"What\" and \"time\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWuzcbsDefpp"
   },
   "source": [
    "![gif_rnn](https://miro.medium.com/max/500/1*1U8H9EZiDqfylJU7Im23Ag.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAzHDVtJefpp"
   },
   "source": [
    "The big difference between classical neural networks and recursive neural networks is that those of RNN can take one or more input vectors and produce one or more output vectors. The output(s) are influenced not only by weights applied to the inputs like a regular NN, but also by a \"hidden\" state vector representing the context based on the previous inputs/outputs. This implies that regular NN must have input and output vectors that have fixed sizes, whereas with RNNs, the input and output must not.\n",
    "\n",
    "\n",
    "The relationships between the vectors can be represented as follows: \n",
    "\n",
    "![rnn_schema](https://i.stack.imgur.com/b4sus.jpg)\n",
    "[source](https://stackoverflow.com/questions/43034960/many-to-one-and-many-to-many-lstm-examples-in-keras)\n",
    "\n",
    "The red rectangles represent the input vectors. The blue rectangles represent the output vectors. The green rectangles are the state vectors.\n",
    "\n",
    "* **one to one :** Representation of a traditional **non-recurrent** Neural Network\n",
    "* **one to many :** A fixed vector as input and vector sequences as output. (Example an image as input and a description of the image as output.)\n",
    "* **many to one :** Sequences of vectors as inputs, and one vector as output. (Example: Sentence classification)\n",
    "* **many to many :** Vector sequences as inputs and vector sequences as outputs. (Sentence translation and/or Name entity recognition.)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPSqemyxcEug"
   },
   "source": [
    "But RNNs have a problem. For one thing, this architecture has a short-term memory. This implies that while state vectors can contain information about neighboring words, this information is limited by distance. This works very well on small sequences (for example for the next 3 or 4 words in a sentence). But if the sequences are long, the gradients (values calculated to tune the network) computed during their training (backpropagation) either vanish (multiplication of many 0 < values < 1) or explode (multiplication of many large values) causing it to train very slowly.\n",
    "\n",
    "To model very long term dependencies, it is necessary to give recurrent neural networks the ability to maintain a state over a long period of time.\n",
    "\n",
    "This is where LSTM (Long Short Term Memory) networks come in. These networks have an internal memory called cell. The cell allows to maintain a state as long as necessary. This cell consists of a numerical value that the network can control according to the situation. This cell can have three control gates, which are activation functions. There is an input gate that decides whether the input should change the content of the cell. There is also a forget gate that decides whether to reset the content of the cell to 0. And finally there is an output gate that decides if the content of the cell should influence the output of the neuron.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Mohamed_Akram_Zaytar/publication/304066008/figure/fig7/AS:494978687746048@1495023523357/A-simple-LSTM-gate-with-only-input-output-and-forget-gates.png)\n",
    "\n",
    "[source](https://www.researchgate.net/figure/A-simple-LSTM-gate-with-only-input-output-and-forget-gates_fig7_304066008)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPTLsbuOmRVx"
   },
   "source": [
    "These operations in the gates allow the LSTM to retain or delete information that it has in memory. For example, in our sentence \"Last night I ate a hamburger and some\", it is important to retain the words \"hamburger\" and \"eat\" while the determiners \"a\", \"and\" can be forgotten by the network.\n",
    "\n",
    "The data stored in the memory of the network is in fact a vector noted $c_t$ : the state of the cell. As this state depends on the previous state $c_{t-1}$, which itself depends on still previous states, the network can keep information that it has seen a long time before (contrary to the classical RNN).\n",
    "\n",
    "\n",
    "[More resources](http://www.diva-portal.org/smash/get/diva2:1216739/FULLTEXT01.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yea12uEQpnuY"
   },
   "source": [
    "| Advantages | Disadvantages|\n",
    "|------------|--------------|\n",
    "| They are able to model long-term sequence dependencies|  They increase the computing complexity compared to the RNN with the introduction of more parameters to learn. |\n",
    "| They are more robust to the problem of short memory than ‘Vanilla’ RNNs since the definition of the internal memory is changed   | The memory required is higher than the one of ‘Vanilla’ RNNs due to the presence of several memory cells. | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz5sTtTEq4vj"
   },
   "source": [
    "**Exercise :** Explain using your own words, (no copy and paste) how RNN works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SEPPRNurk68"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eviN5d8HtK_Q"
   },
   "source": [
    "### Transformer (2017 - present)\n",
    "\n",
    "Natural language processing is, today, essentially dominated by sequence-to-sequence (or seq2seq) models. A seq2seq model is a model that takes a sequence (a sequence of elements of the same type) as input and returns a sequence as output. The example par excellence for this type of model is the translation of text. Among the seq2seq models that have emerged in the last few years, if there is one that stands out, it is the Transformer. The Transformer is a sequence-to-sequence model based on the attention mechanism and not on a recurrent neural network as it was the case for the previous models. On the other hand, we will keep the sequences as inputs and outputs. \n",
    "\n",
    "\n",
    "The Attention mechanism is a measure of how well two elements in two sequences are related. In a sequence-to-sequence context in NLP, the self-attention mechanism is used to determine which word or sequence of words in the entire sequence gives context elements when processing a word. It thus makes it possible to capture the relationships between words, even if they are far apart from each other in the sequence.\n",
    "\n",
    "![](https://i.imgur.com/PHWQnbX.png)  \n",
    "[source](https://www.kaggle.com/residentmario/transformer-architecture-self-attention)\n",
    "\n",
    "* [more resources](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "\n",
    "A transformer consists of two parts, an encoder and a decoder. The encoder is a neural network used to transform the input sequence into a vector representation of the sequence.  The head of attention mechanism then captures for each word the context elements relevant to it and integrates it into the vector generated by the encoder. This step is repeated several times simultaneously for all the words, thus parallelizing the process. The final vector representation thus generated by the encoder then serves as input to a second network, the decoder, which is used to generate words sequentially.\n",
    "\n",
    "![encode-decoder](https://miro.medium.com/max/1284/1*1BFAQXkNiLySIhB__24EkQ.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tULFn5KQaFAM"
   },
   "source": [
    "There are several models using the transformer architecture, such as Bert, XLnet, or GPT-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ka-9MDaprt-U"
   },
   "source": [
    "**Exercise :** Explain using your own words (no copy and paste) what a layer of attention is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKhcMkL5rrm6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CK9l-2ggefpr"
   },
   "source": [
    "## In conclusion\n",
    "\n",
    "For many NLP problems, a statistical language model is required. Models based on neural networks offer the best results, thanks in particular to their generalization capability. As for the transformers type models that have recently appeared, they allow to reach very good performances on some NLP tasks with limited data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good to know it. note_yuri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.twitter import json2csv\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "input_file = twitter_samples.abspath(\"tweets.20150430-223406.json\")\n",
    "\n",
    "with open(input_file) as fp:   # save the dataset as csv for working with DataFrames\n",
    "    json2csv(fp, 'tweets_text.csv', ['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pformat\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('tweets_text.csv')\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "timeline_of_nlp.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('myNLTK')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6e93389139a38db427026b9637522739d6fe06c48619f3fde42a7ea95a9cf1c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
